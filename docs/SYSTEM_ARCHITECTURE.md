# üèóÔ∏è SaralPolicy System Architecture

**Version:** 1.0  
**Last Updated:** January 2025  
**Author:** SaralPolicy Team

---

## üìã Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [System Components](#system-components)
3. [Data Flow](#data-flow)
4. [Component Details](#component-details)
5. [Integration Points](#integration-points)
6. [Performance & Scalability](#performance--scalability)
7. [Security & Privacy](#security--privacy)

---

## Architecture Overview

SaralPolicy is a **privacy-first, locally-run AI system** for analyzing Indian insurance policy documents. The architecture is designed around the principle of **zero cloud dependencies** and **complete user data privacy**.

### Core Design Principles

‚úÖ **Privacy-First:** All processing happens locally on the user's machine  
‚úÖ **Modular Architecture:** Loosely coupled services for flexibility  
‚úÖ **Offline Capable:** No internet required for core functionality  
‚úÖ **Production-Ready:** Built-in guardrails, evaluation, and HITL workflows  
‚úÖ **Regulatory Compliance:** IRDAI knowledge base integration  

---

## System Components

```mermaid
graph TB
    subgraph FE["üé® FRONTEND LAYER"]
        UI["<b>Material 3 Web UI</b><br/>‚Ä¢ Drag & Drop Upload<br/>‚Ä¢ Q&A Interface<br/>‚Ä¢ Responsive Design"]
    end

    subgraph API_LAYER["‚ö° API LAYER"]
        API["<b>FastAPI Server</b><br/>‚Ä¢ Request Routing<br/>‚Ä¢ Session Management<br/>‚Ä¢ Response Formatting"]
    end

    subgraph CORE["üîß CORE SERVICES"]
        DOC["<b>Document Processor</b><br/>‚Ä¢ PyPDF2 (Parallel)<br/>‚Ä¢ python-docx<br/>‚Ä¢ Multi-format Support"]
        RAG["<b>RAG Service</b><br/>‚Ä¢ Hybrid Search<br/>‚Ä¢ Context Retrieval<br/>‚Ä¢ Prompt Engineering"]
        LLM["<b>Ollama LLM</b><br/>‚Ä¢ Gemma 3 4B (Local)<br/>‚Ä¢ Zero Cloud Calls<br/>‚Ä¢ Privacy-First"]
    end

    subgraph STORAGE["üíæ KNOWLEDGE & STORAGE"]
        EMBED["<b>Embeddings</b><br/>nomic-embed-text<br/>274MB via Ollama"]
        CHROMA[("<b>ChromaDB</b><br/>Vector Store<br/>Persistent Storage")]
        IRDAI[("<b>IRDAI Knowledge</b><br/>39 Regulatory Chunks<br/>Pre-indexed")]
    end

    subgraph SEARCH["üîç SEARCH COMPONENTS"]
        BM25["<b>BM25 Search</b><br/>Keyword Matching<br/>rank-bm25"]
        VSEARCH["<b>Vector Search</b><br/>Semantic Similarity<br/>ChromaDB Queries"]
    end

    subgraph SAFETY["üõ°Ô∏è SAFETY & QUALITY"]
        GUARD["<b>Guardrails</b><br/>‚Ä¢ PII Protection<br/>‚Ä¢ Input Validation<br/>‚Ä¢ Hallucination Check"]
        EVAL["<b>Evaluation</b><br/>‚Ä¢ TruLens Metrics<br/>‚Ä¢ Giskard Tests<br/>‚Ä¢ DeepEval"]
        HITL["<b>HITL System</b><br/>‚Ä¢ Expert Review<br/>‚Ä¢ Low Confidence<br/>‚Ä¢ Quality Assurance"]
    end

    subgraph AUX["üîä AUXILIARY SERVICES"]
        TTS["<b>Text-to-Speech</b><br/>pyttsx3, gTTS"]
        TRANS["<b>Translation</b><br/>Hindi/English<br/>googletrans"]
    end

    %% Primary User Flow (Bold)
    UI ====>|"1. Upload<br/>Document"| API
    API ====>|"2. Extract<br/>Text"| DOC
    DOC ====>|"3. Index &<br/>Embed"| RAG
    RAG ====>|"4. Retrieve<br/>Context"| CHROMA
    RAG ====>|"5. Generate<br/>Prompt"| LLM
    LLM ====>|"6. Return<br/>Analysis"| API
    API ====>|"7. Display<br/>Results"| UI

    %% Document Processing
    DOC -->|Generate Embeddings| EMBED
    EMBED -->|Store Vectors| CHROMA
    IRDAI -.->|Pre-loaded| CHROMA

    %% RAG Search Pipeline
    RAG -->|Keyword Query| BM25
    RAG -->|Semantic Query| VSEARCH
    BM25 -->|Results| CHROMA
    VSEARCH -->|Results| CHROMA
    IRDAI -.->|Augment Context| RAG

    %% Safety & Quality
    API -->|Validate| GUARD
    LLM -->|Evaluate| EVAL
    EVAL -->|If Low Confidence| HITL
    GUARD -->|Block/Flag| API
    HITL -->|Expert Verified| API

    %% User Interactions
    UI -->|Ask Questions| API
    API -->|Q&A via RAG| RAG

    %% Output Enhancement
    API -->|Generate Audio| TTS
    API -->|Translate Text| TRANS
    TTS -->|Audio File| UI
    TRANS -->|Hindi Output| UI

    %% Styling with High Contrast
    classDef frontendStyle fill:#0d47a1,stroke:#ffffff,stroke-width:3px,color:#ffffff
    classDef apiStyle fill:#4a148c,stroke:#ffffff,stroke-width:3px,color:#ffffff
    classDef coreStyle fill:#e65100,stroke:#ffffff,stroke-width:3px,color:#ffffff
    classDef storageStyle fill:#1b5e20,stroke:#ffffff,stroke-width:3px,color:#ffffff
    classDef searchStyle fill:#006064,stroke:#ffffff,stroke-width:3px,color:#ffffff
    classDef safetyStyle fill:#b71c1c,stroke:#ffffff,stroke-width:3px,color:#ffffff
    classDef auxStyle fill:#880e4f,stroke:#ffffff,stroke-width:3px,color:#ffffff
    classDef subgraphStyle fill:#f5f5f5,stroke:#424242,stroke-width:2px

    class UI frontendStyle
    class API apiStyle
    class DOC,RAG,LLM coreStyle
    class EMBED,CHROMA,IRDAI storageStyle
    class BM25,VSEARCH searchStyle
    class GUARD,EVAL,HITL safetyStyle
    class TTS,TRANS auxStyle
    class FE,API_LAYER,CORE,STORAGE,SEARCH,SAFETY,AUX subgraphStyle
```

---

## Data Flow

### 1. Document Upload Flow

```
User ‚Üí Frontend UI ‚Üí FastAPI ‚Üí Document Processor ‚Üí Text Extraction
                                         ‚Üì
                                   Embedding Generation
                                         ‚Üì
                                   Vector Storage (ChromaDB)
```

**Processing Steps:**
1. **Upload:** User drags PDF/DOCX to web interface
2. **Validation:** File type, size, and content checks via Guardrails
3. **Extraction:** Parallel text extraction using PyPDF2 (multi-threaded)
4. **Chunking:** Intelligent text chunking for optimal RAG performance
5. **Embedding:** Generate embeddings via Ollama's nomic-embed-text
6. **Storage:** Store vectors + metadata in ChromaDB persistent storage

**Performance Optimization:**
- ‚úÖ MD5-based document caching (avoid reprocessing)
- ‚úÖ Parallel PDF page extraction (4-worker ThreadPoolExecutor)
- ‚úÖ Batch embedding generation
- ‚úÖ Optimized chunking with list comprehensions

---

### 2. Policy Analysis Flow

```
Uploaded Document ‚Üí RAG Service ‚Üí Hybrid Search (BM25 + Vector)
                                       ‚Üì
                            Context Retrieval from IRDAI KB
                                       ‚Üì
                            Prompt Engineering with Context
                                       ‚Üì
                            LLM Generation (Gemma 3 4B)
                                       ‚Üì
                            Evaluation & Quality Check
                                       ‚Üì
                    High Confidence ‚Üí User | Low Confidence ‚Üí HITL Review
```

**Key Features:**
- **Hybrid Search:** Combines keyword (BM25) + semantic (vector) search
- **Context Augmentation:** IRDAI knowledge base pre-indexed (39 regulatory chunks)
- **Quality Control:** TruLens, Giskard, DeepEval metrics
- **Human Oversight:** Automatic flagging for expert review

---

### 3. Q&A Interaction Flow

```
User Question ‚Üí API ‚Üí Guardrails (Input Validation)
                          ‚Üì
                    RAG Query (Hybrid Search)
                          ‚Üì
                    Context from Document + IRDAI KB
                          ‚Üì
                    LLM Generation (Contextual Answer)
                          ‚Üì
                    PII Redaction & Safety Check
                          ‚Üì
                    Response + Sources ‚Üí User
```

**Optimizations:**
- ‚úÖ Query caching (MD5-based keys)
- ‚úÖ Connection pooling for Ollama API
- ‚úÖ Persistent ChromaDB sessions

---

## Component Details

### üé® Frontend Layer

**Technology:** Material 3 Design, HTML5, CSS3, JavaScript  
**Features:**
- Drag-and-drop file upload
- Real-time analysis progress indicators
- Interactive Q&A chat interface
- Audio playback for TTS summaries
- Dark mode support
- Print-friendly policy views

**File:** `backend/templates/index.html`, `backend/static/`

---

### ‚ö° API Layer

**Technology:** FastAPI (Python 3.10+)  
**Key Endpoints:**

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | Serve frontend UI |
| `/upload` | POST | Upload policy document |
| `/analyze` | POST | Analyze uploaded policy |
| `/rag/ask` | POST | Ask question via RAG |
| `/rag/stats` | GET | RAG service statistics |
| `/tts` | POST | Generate audio summary |

**Features:**
- CORS middleware for cross-origin requests
- Session management for multi-user support
- Structured logging (structlog)
- Performance metrics tracking

**File:** `backend/main.py`

---

### üîß Core Services

#### 1. Document Processor

**Purpose:** Extract text from PDF, DOCX, TXT files  
**Optimizations:**
- Parallel PDF page processing (ThreadPoolExecutor)
- MD5-based file caching
- Memory-efficient streaming for large files

**File:** `backend/main.py` (SaralPolicyLocal class)

**Supported Formats:**
- ‚úÖ PDF (via PyPDF2)
- ‚úÖ DOCX (via python-docx)
- ‚úÖ TXT (native Python)

---

#### 2. RAG Service

**Purpose:** Retrieval-Augmented Generation with hybrid search  
**Technology:** ChromaDB + BM25 + Ollama embeddings

**Key Features:**
- **Hybrid Search:** Combines BM25 (keyword) + Vector (semantic) search
- **Batch Processing:** Parallel embedding generation with caching
- **Connection Pooling:** Persistent HTTP sessions for Ollama
- **Query Caching:** MD5-based cache for repeated queries

**File:** `backend/app/services/rag_service.py`

**Methods:**
```python
index_document(text, metadata)  # Index document chunks
hybrid_search(query, collection_name, top_k)  # Search both BM25 + Vector
get_embeddings(texts)  # Batch embedding with cache
get_stats()  # Service statistics
```

---

#### 3. Ollama LLM Service

**Purpose:** Local LLM inference using Gemma 3 4B  
**Model:** `gemma3:4b` (4 billion parameters)

**Configuration:**
- **Temperature:** 0.3 (deterministic)
- **Context Window:** 4096 tokens
- **Max Tokens:** 1500 output
- **Streaming:** Disabled (batch processing)

**Privacy Guarantee:** 
- ‚úÖ 100% local inference
- ‚úÖ No API keys required
- ‚úÖ No data sent to cloud services

**File:** `backend/app/services/ollama_llm_service.py`

---

### üíæ Knowledge & Storage

#### ChromaDB Vector Store

**Purpose:** Persistent vector storage for embeddings  
**Location:** `backend/data/chroma/`

**Collections:**
- `policy_documents` - Uploaded policy chunks
- `irdai_knowledge_base` - Pre-indexed regulatory content

**Metadata Schema:**
```json
{
  "chunk_id": "string",
  "source": "filename.pdf",
  "chunk_index": 0,
  "type": "policy_section",
  "timestamp": "2025-01-07T12:00:00"
}
```

---

#### IRDAI Knowledge Base

**Purpose:** Regulatory compliance context  
**Location:** `backend/data/irdai_knowledge/`

**Content:**
- `IRDAI_Master_Circular_Health_2024.txt` (Health insurance regulations)
- `IRDAI_Protection_of_Policyholders_Interests.txt` (Consumer rights)
- `Insurance_Guidelines_Terms_Definitions.txt` (Standard terminology)

**Statistics:**
- 39 indexed chunks
- Pre-embedded and ready for queries
- Automatically loaded on service startup

---

### üîç Search Components

#### BM25 Keyword Search

**Purpose:** Lexical matching for exact term searches  
**Library:** `rank-bm25`

**Use Cases:**
- Policy number lookups
- Specific clause references
- Exact terminology searches

---

#### Vector Semantic Search

**Purpose:** Semantic similarity matching  
**Embedding Model:** `nomic-embed-text` (274MB via Ollama)

**Use Cases:**
- Conceptual queries ("What is covered for accidents?")
- Cross-language understanding
- Paraphrase detection

---

### üõ°Ô∏è Safety & Quality

#### 1. Guardrails Service

**Purpose:** Input validation, PII protection, hallucination prevention  
**File:** `backend/app/services/guardrails_service.py`

**Checks:**
- ‚úÖ PII redaction (names, phone, Aadhaar, PAN)
- ‚úÖ Input sanitization (SQL injection, XSS)
- ‚úÖ File size and type validation
- ‚úÖ Prompt injection detection

---

#### 2. Evaluation Frameworks

**Purpose:** Quality metrics for LLM outputs  
**File:** `backend/app/services/evaluation.py`

**Frameworks:**
- **TruLens:** Context relevance, answer relevance, groundedness
- **Giskard:** Hallucination detection, robustness testing
- **DeepEval:** Answer correctness, faithfulness metrics

**Thresholds:**
- High Confidence: Score ‚â• 0.75
- Medium Confidence: Score 0.50-0.74
- Low Confidence: Score < 0.50 ‚Üí Trigger HITL

---

#### 3. Human-in-the-Loop (HITL)

**Purpose:** Expert review for low-confidence analyses  
**File:** `backend/app/services/hitl_service.py`

**Workflow:**
1. System flags low-confidence result
2. Expert reviews analysis in UI
3. Expert approves/corrects/rejects
4. Feedback stored for model improvement
5. User receives verified analysis

---

### üîä Auxiliary Services

#### Text-to-Speech (TTS)

**Purpose:** Generate audio summaries  
**Libraries:** pyttsx3 (offline), gTTS (online fallback)

**Features:**
- Hindi + English voice support
- Adjustable speech rate
- MP3 output format

**File:** `backend/app/services/tts_service.py`

---

#### Translation Service

**Purpose:** Hindi ‚Üî English translation  
**Library:** googletrans (unofficial API)

**Use Cases:**
- Bilingual policy summaries
- Term explanations in Hindi
- User interface localization

**File:** `backend/app/services/translation_service.py`

---

## Integration Points

### External Dependencies

1. **Ollama** (Required)
   - Installation: `curl https://ollama.ai/install.sh | sh`
   - Models: `gemma3:4b`, `nomic-embed-text`
   - Port: 11434 (default)

2. **ChromaDB** (Bundled)
   - Version: 0.5.15
   - Storage: `backend/data/chroma/`

3. **Python Packages** (See `requirements.txt`)
   - FastAPI, Uvicorn
   - PyPDF2, python-docx
   - rank-bm25, chromadb
   - pyttsx3, googletrans

---

## Performance & Scalability

### Current Performance Metrics

| Operation | Time (Avg) | Optimization |
|-----------|------------|--------------|
| PDF Parsing (10 pages) | 2.3s | Parallel processing |
| Embedding Generation (50 chunks) | 1.8s | Batch API calls |
| Hybrid Search Query | 0.4s | Query caching |
| LLM Generation (500 tokens) | 3.5s | Optimized prompt |
| Full Analysis | 8-12s | End-to-end pipeline |

### Scalability Considerations

**Current POC Limitations:**
- Single-user session management
- In-memory caching (lost on restart)
- No distributed processing

**Production Roadmap:**
- Multi-user support with session persistence
- Distributed vector store (Weaviate, Milvus)
- GPU acceleration for embeddings
- Load balancing for API layer

---

## Security & Privacy

### Privacy Guarantees

‚úÖ **Zero Cloud Calls:** All AI processing happens locally  
‚úÖ **No API Keys:** No third-party AI services  
‚úÖ **Data Sovereignty:** User data never leaves their machine  
‚úÖ **PII Protection:** Automatic redaction of sensitive info  
‚úÖ **Audit Logs:** All operations logged locally  

### Security Measures

1. **Input Validation:** All uploads sanitized via Guardrails
2. **File Type Restrictions:** Only PDF/DOCX/TXT allowed
3. **Size Limits:** Max 10MB upload size (configurable)
4. **SQL Injection Prevention:** Parameterized queries only
5. **XSS Protection:** Output sanitization in frontend

---

## Technology Stack

### Backend
- **Framework:** FastAPI 0.115.12
- **Language:** Python 3.10+
- **AI/ML:** Ollama (Gemma 3 4B, nomic-embed-text)
- **Vector DB:** ChromaDB 0.5.15
- **Search:** rank-bm25 0.2.2

### Frontend
- **UI Framework:** Material Design 3
- **Styling:** Custom CSS with dark mode
- **Interactivity:** Vanilla JavaScript

### Infrastructure
- **Server:** Uvicorn ASGI
- **Logging:** structlog
- **Testing:** pytest, unittest

---

## Quick Reference

### Service Endpoints

```bash
# Health check
curl http://localhost:8000/

# Upload document
curl -X POST http://localhost:8000/upload \
  -F "file=@policy.pdf"

# Analyze policy
curl -X POST http://localhost:8000/analyze \
  -F "file=@policy.pdf"

# Ask question via RAG
curl -X POST http://localhost:8000/rag/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What is the sum insured?", "use_knowledge_base": true}'

# Get RAG statistics
curl http://localhost:8000/rag/stats
```

### File Locations

| Component | Path |
|-----------|------|
| Main App | `backend/main.py` |
| RAG Service | `backend/app/services/rag_service.py` |
| Ollama LLM | `backend/app/services/ollama_llm_service.py` |
| ChromaDB Data | `backend/data/chroma/` |
| IRDAI Docs | `backend/data/irdai_knowledge/` |
| Frontend | `backend/templates/index.html` |
| Tests | `tests/` |

---

## Next Steps

### Immediate Enhancements
1. Add Automatic Speech Recognition (ASR) for voice queries
2. Implement Redis for distributed caching
3. Add PostgreSQL for persistent session management
4. Integrate more IRDAI documents (target: 100+ chunks)

### Long-Term Vision
1. Multi-language support (10+ Indian languages)
2. Mobile app (React Native)
3. Browser extension for policy scanning
4. API marketplace for insurtech partners

---

**For questions or contributions, see:** [CONTRIBUTING.md](../CONTRIBUTING.md)

**Last Updated:** January 7, 2025  
**Version:** 1.0.0
